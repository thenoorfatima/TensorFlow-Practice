{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of transformer_asr","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/audio/ipynb/transformer_asr.ipynb","timestamp":1617515834575}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"n1rMU8u8dMfL"},"source":["# Automatic Speech Recognition with Transformer\n","\n","**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n","**Date created:** 2021/01/13<br>\n","**Last modified:** 2021/01/13<br>\n","**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."]},{"cell_type":"markdown","metadata":{"id":"63gQbA02dMfY"},"source":["## Introduction\n","\n","Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n","ASR can be treated as a sequence-to-sequence problem, where the\n","audio can be represented as a sequence of feature vectors\n","and the text as a sequence of characters, words, or subword tokens.\n","\n","For this demonstration, we will use the LJSpeech dataset from the\n","[LibriVox](https://librivox.org/) project. It consists of short\n","audio clips of a single speaker reading passages from 7 non-fiction books.\n","Our model will be similar to the original Transformer (both encoder and decoder)\n","as proposed in the paper, \"Attention is All You Need\".\n","\n","\n","**References:**\n","\n","- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n","- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n","- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n","- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"]},{"cell_type":"code","metadata":{"id":"2BWp8l57dMfZ","executionInfo":{"status":"ok","timestamp":1617517720288,"user_tz":-330,"elapsed":2541,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","import os\n","import random\n","from glob import glob\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNjxsINedMfa"},"source":["## Define the Transformer Input Layer\n","\n","When processing past target tokens for the decoder, we compute the sum of\n","position embeddings and token embeddings.\n","\n","When processing audio features, we apply convolutional layers to downsample\n","them (via convolution stides) and process local relationships."]},{"cell_type":"code","metadata":{"id":"dWCaeXuidMfb","executionInfo":{"status":"ok","timestamp":1617517720297,"user_tz":-330,"elapsed":2525,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class TokenEmbedding(layers.Layer):\n","    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n","        super().__init__()\n","        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.emb(x)\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","\n","class SpeechFeatureEmbedding(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return self.conv3(x)\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lz33YdxdMfc"},"source":["## Transformer Encoder Layer"]},{"cell_type":"code","metadata":{"id":"1YYW18B0dMfc","executionInfo":{"status":"ok","timestamp":1617517720298,"user_tz":-330,"elapsed":2520,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8Adt75RdMfd"},"source":["## Transformer Decoder Layer"]},{"cell_type":"code","metadata":{"id":"XM75lXKydMfd","executionInfo":{"status":"ok","timestamp":1617517720299,"user_tz":-330,"elapsed":2515,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n","        super().__init__()\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.self_att = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.self_dropout = layers.Dropout(0.5)\n","        self.enc_dropout = layers.Dropout(0.1)\n","        self.ffn_dropout = layers.Dropout(0.1)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n","        \"\"\"Masks the upper half of the dot product matrix in self attention.\n","\n","        This prevents flow of information from future tokens to current token.\n","        1's in the lower triangle, counting from the lower right corner.\n","        \"\"\"\n","        i = tf.range(n_dest)[:, None]\n","        j = tf.range(n_src)\n","        m = i >= j - n_src + n_dest\n","        mask = tf.cast(m, dtype)\n","        mask = tf.reshape(mask, [1, n_dest, n_src])\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, enc_out, target):\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        target_att = self.self_att(target, target, attention_mask=causal_mask)\n","        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n","        enc_out = self.enc_att(target_norm, enc_out)\n","        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n","        ffn_out = self.ffn(enc_out_norm)\n","        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n","        return ffn_out_norm\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gEouSkafdMfe"},"source":["## Complete the Transformer model\n","\n","Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n","During training, we give the decoder the target character sequence shifted to the left\n","as input. During inference, the decoder uses its own past predictions to predict the\n","next token."]},{"cell_type":"code","metadata":{"id":"FtZF-2IndMfg","executionInfo":{"status":"ok","timestamp":1617517720300,"user_tz":-330,"elapsed":2511,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class Transformer(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=10,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n","        return y\n","\n","    def call(self, inputs):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source)\n","        y = self.decode(x, target)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def test_step(self, batch):\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTh27MLGdMfj"},"source":["## Download the dataset\n","\n","Note: This requires ~3.6 GB of disk space and\n","takes ~5 minutes for the extraction of files."]},{"cell_type":"code","metadata":{"id":"PuH8ANlfdMfk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617518036394,"user_tz":-330,"elapsed":318601,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}},"outputId":"4678ab46-fad1-47fa-9fe7-36cff5915877"},"source":["keras.utils.get_file(\n","    os.path.join(os.getcwd(), \"data.tar.gz\"),\n","    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n","    extract=True,\n","    archive_format=\"tar\",\n","    cache_dir=\".\",\n",")\n","\n","\n","saveto = \"./datasets/LJSpeech-1.1\"\n","wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n","\n","id_to_text = {}\n","with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n","    for line in f:\n","        id = line.strip().split(\"|\")[0]\n","        text = line.strip().split(\"|\")[2]\n","        id_to_text[id] = text\n","\n","\n","def get_data(wavs, id_to_text, maxlen=50):\n","    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n","    data = []\n","    for w in wavs:\n","        id = w.split(\"/\")[-1].split(\".\")[0]\n","        if len(id_to_text[id]) < maxlen:\n","            data.append({\"audio\": w, \"text\": id_to_text[id]})\n","    return data\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n","2748579840/2748572632 [==============================] - 38s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5jm8V2MedMfm"},"source":["## Preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"NRMv7d7QdMfn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617518048365,"user_tz":-330,"elapsed":330557,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}},"outputId":"1fddf71a-f104-49e2-b9f8-54d9f451bc49"},"source":["\n","class VectorizeChar:\n","    def __init__(self, max_len=50):\n","        self.vocab = (\n","            [\"-\", \"#\", \"<\", \">\"]\n","            + [chr(i + 96) for i in range(1, 27)]\n","            + [\" \", \".\", \",\", \"?\"]\n","        )\n","        self.max_len = max_len\n","        self.char_to_idx = {}\n","        for i, ch in enumerate(self.vocab):\n","            self.char_to_idx[ch] = i\n","\n","    def __call__(self, text):\n","        text = text.lower()\n","        text = text[: self.max_len - 2]\n","        text = \"<\" + text + \">\"\n","        pad_len = self.max_len - len(text)\n","        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n","\n","    def get_vocabulary(self):\n","        return self.vocab\n","\n","\n","max_target_len = 200  # all transcripts in out data are < 200 characters\n","data = get_data(wavs, id_to_text, max_target_len)\n","vectorizer = VectorizeChar(max_target_len)\n","print(\"vocab size\", len(vectorizer.get_vocabulary()))\n","\n","\n","def create_text_ds(data):\n","    texts = [_[\"text\"] for _ in data]\n","    text_ds = [vectorizer(t) for t in texts]\n","    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n","    return text_ds\n","\n","\n","def path_to_audio(path):\n","    # spectrogram using stft\n","    audio = tf.io.read_file(path)\n","    audio, _ = tf.audio.decode_wav(audio, 1)\n","    audio = tf.squeeze(audio, axis=-1)\n","    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n","    x = tf.math.pow(tf.abs(stfts), 0.5)\n","    # normalisation\n","    means = tf.math.reduce_mean(x, 1, keepdims=True)\n","    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n","    x = (x - means) / stddevs\n","    audio_len = tf.shape(x)[0]\n","    # padding to 10 seconds\n","    pad_len = 2754\n","    paddings = tf.constant([[0, pad_len], [0, 0]])\n","    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n","    return x\n","\n","\n","def create_audio_ds(data):\n","    flist = [_[\"audio\"] for _ in data]\n","    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n","    audio_ds = audio_ds.map(\n","        path_to_audio, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    return audio_ds\n","\n","\n","def create_tf_dataset(data, bs=4):\n","    audio_ds = create_audio_ds(data)\n","    text_ds = create_text_ds(data)\n","    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n","    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n","    ds = ds.batch(bs)\n","    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n","    return ds\n","\n","\n","split = int(len(data) * 0.99)\n","train_data = data[:split]\n","test_data = data[split:]\n","ds = create_tf_dataset(train_data, bs=64)\n","val_ds = create_tf_dataset(test_data, bs=4)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["vocab size 34\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yffVOAAPdMfn"},"source":["## Callbacks to display predictions"]},{"cell_type":"code","metadata":{"id":"bLKzNqkmdMfo","executionInfo":{"status":"ok","timestamp":1617518048368,"user_tz":-330,"elapsed":330555,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class DisplayOutputs(keras.callbacks.Callback):\n","    def __init__(\n","        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n","    ):\n","        \"\"\"Displays a batch of outputs after every epoch\n","\n","        Args:\n","            batch: A test batch containing the keys \"source\" and \"target\"\n","            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n","            target_start_token_idx: A start token index in the target vocabulary\n","            target_end_token_idx: An end token index in the target vocabulary\n","        \"\"\"\n","        self.batch = batch\n","        self.target_start_token_idx = target_start_token_idx\n","        self.target_end_token_idx = target_end_token_idx\n","        self.idx_to_char = idx_to_token\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch % 5 != 0:\n","            return\n","        source = self.batch[\"source\"]\n","        target = self.batch[\"target\"].numpy()\n","        bs = tf.shape(source)[0]\n","        preds = self.model.generate(source, self.target_start_token_idx)\n","        preds = preds.numpy()\n","        for i in range(bs):\n","            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n","            prediction = \"\"\n","            for idx in preds[i, :]:\n","                prediction += self.idx_to_char[idx]\n","                if idx == self.target_end_token_idx:\n","                    break\n","            print(f\"target:     {target_text.replace('-','')}\")\n","            print(f\"prediction: {prediction}\\n\")\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLCy8ThidMfo"},"source":["## Learning rate schedule"]},{"cell_type":"code","metadata":{"id":"PFvAm7BudMfp","executionInfo":{"status":"ok","timestamp":1617518048369,"user_tz":-330,"elapsed":330550,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}}},"source":["\n","class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(\n","        self,\n","        init_lr=0.00001,\n","        lr_after_warmup=0.001,\n","        final_lr=0.00001,\n","        warmup_epochs=15,\n","        decay_epochs=85,\n","        steps_per_epoch=203,\n","    ):\n","        super().__init__()\n","        self.init_lr = init_lr\n","        self.lr_after_warmup = lr_after_warmup\n","        self.final_lr = final_lr\n","        self.warmup_epochs = warmup_epochs\n","        self.decay_epochs = decay_epochs\n","        self.steps_per_epoch = steps_per_epoch\n","\n","    def calculate_lr(self, epoch):\n","        \"\"\" linear warm up - linear decay \"\"\"\n","        warmup_lr = (\n","            self.init_lr\n","            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n","        )\n","        decay_lr = tf.math.maximum(\n","            self.final_lr,\n","            self.lr_after_warmup\n","            - (epoch - self.warmup_epochs)\n","            * (self.lr_after_warmup - self.final_lr)\n","            / (self.decay_epochs),\n","        )\n","        return tf.math.minimum(warmup_lr, decay_lr)\n","\n","    def __call__(self, step):\n","        epoch = step // self.steps_per_epoch\n","        return self.calculate_lr(epoch)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Mjx1_dCdMfq"},"source":["## Create & train the end-to-end model"]},{"cell_type":"code","metadata":{"id":"KrIGr5NmdMfq","colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"status":"error","timestamp":1617518306577,"user_tz":-330,"elapsed":588753,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"}},"outputId":"249197b9-3a25-43c0-d889-406d1c743db6"},"source":["batch = next(iter(val_ds))\n","\n","# The vocabulary to convert predicted indices into characters\n","idx_to_char = vectorizer.get_vocabulary()\n","display_cb = DisplayOutputs(\n","    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",")  # set the arguments as per vocabulary index for '<' and '>'\n","\n","model = Transformer(\n","    num_hid=200,\n","    num_head=2,\n","    num_feed_forward=400,\n","    target_maxlen=max_target_len,\n","    num_layers_enc=4,\n","    num_layers_dec=1,\n","    num_classes=34,\n",")\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(\n","    from_logits=True, label_smoothing=0.1,\n",")\n","\n","learning_rate = CustomSchedule(\n","    init_lr=0.00001,\n","    lr_after_warmup=0.001,\n","    final_lr=0.00001,\n","    warmup_epochs=15,\n","    decay_epochs=85,\n","    steps_per_epoch=len(ds),\n",")\n","optimizer = keras.optimizers.Adam(learning_rate)\n","model.compile(optimizer=optimizer, loss=loss_fn)\n","\n","history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["144/203 [====================>.........] - ETA: 1:29 - loss: 1.7951"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a50bedeeafd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Y8Zo6ERRdMfr"},"source":["In practice, you should train for around 100 epochs or more.\n","\n","Some of the predicted text at or around epoch 35 may look as follows:\n","```\n","target:     <as they sat in the car, frazier asked oswald where his lunch was>\n","prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n","\n","target:     <under the entry for may one, nineteen sixty,>\n","prediction: <under the introus for may monee, nin the sixty,>\n","```"]}]}